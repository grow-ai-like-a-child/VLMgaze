<!DOCTYPE html>
<html>
<head>
  <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëÅÔ∏è</text></svg>">
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing.">
  <meta property="og:title" content="Can Vision Language Models Infer Human Gaze Direction? A Controlled Study"/>
  <meta property="og:description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing."> 
  <meta property="og:url" content="https://grow-ai-like-a-child.github.io/VLMgaze/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser_v2.jpg"/>
  <meta property="og:image:width" content="1661"/>
  <meta property="og:image:height" content="649"/>

  <meta name="twitter:title" content="VLMs Cannot Yet Infer Human Gaze Direction">
  <meta name="twitter:description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing."> 
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser_v2.jpg">
  <meta name="twitter:card" content="static/images/teaser_v2.jpg">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Models, Gaze Inference, Human Vision, Spatial Reasoning, AI Evaluation">
  <meta name="author" content="Zory Zhang, Pinyuan Feng, Bingyang Wang, Tianwei Zhao, Suyang Yu, Qingying Gao, Hokin Deng, Ziqiao Ma, Yijiang Li, Dezhi Luo (GrowAI Team)">
  <meta name="robots" content="index, follow">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Can Vision Language Models Infer Human Gaze Direction? A Controlled Study</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëÅÔ∏è</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.0/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.0/dist/js/bulma-slider.min.js"></script>
  
  <style>
    #survey-carousel {
        overflow: hidden !important;
    }
    
    .results-carousel {
        margin-bottom: 3rem !important;
        overflow: hidden !important;
    }
    
    .results-carousel .slider-pagination {
        margin-top: 2rem !important;
    }

    .results-carousel .item {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        margin: 5px;
        overflow: hidden;
        padding: 20px;
        font-size: 0;
    }

    .results-carousel video {
        margin: 0;
    }
    
    .results-carousel .item > img {
        margin: 0 auto;
        max-height: 450px;
        object-fit: contain;
    }

    .results-carousel .item .subtitle {
    margin-top: 15px;
    margin-bottom: 3rem !important; /* Add more space below subtitle */
    }

    .slider-pagination .slider-page {
        background: #000000;
    }

    .hero-body {
      padding: 3rem 1.5rem;
    }

    .title.is-1 {
      font-size: 2.5rem !important;
    }

    @media screen and (min-width: 1024px) {
      .title.is-1 {
        font-size: 3rem !important;
      }
    }

    .publication-title {
      margin-bottom: 1rem !important;
    }

    .author-block {
      margin-right: 1rem;
    }
  </style>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 20px;">
              <h1 class="title is-1 publication-title">üëÅÔ∏è Can Vision Language Models Infer Human Gaze Direction? A Controlled Study</h1>
            </div>
            <h4 class="title is-size-4 publication-title">Preprint under review</h4>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="#" target="_blank">Zory Zhang</a><sup>‚Ä†,1</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Pinyuan Feng</a><sup>‚Ä†,2</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Bingyang Wang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Tianwei Zhao</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Suyang Yu</a><sup>5</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="#" target="_blank">Qingying Gao</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Hokin Deng</a><sup>*,6</sup>,
              </span>
              <span class="author-block">
                <a href="https://mars-tin.github.io" target="_blank">Ziqiao Ma</a><sup>*,7</sup>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Yijiang Li</a><sup>*,8</sup>,
              </span>
              <span class="author-block">
                <a href="https://ihzedoul.com" target="_blank">Dezhi Luo</a><sup>*,7</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Brown University,
                <sup>2</sup>Columbia University,
                <sup>3</sup>Emory University,
                <sup>4</sup>Johns Hopkins University,<br>
                <sup>5</sup>University of Washington,
                <sup>6</sup>Carnegie Mellon University,
                <sup>7</sup>University of Michigan,
                <sup>8</sup>UC San Diego
              </span>
              <span class="eql-cntrb"><small><br><sup>‚Ä†</sup>Equal Contribution, <sup>*</sup>Equal Advising</small></span>
              <br>
              <span class="author-block" style="color: #4a4a4a; margin-top: 10px;">
                üå± <strong>GrowAI Team</strong> - <a href="https://growing-ai-like-a-child.github.io/" target="_blank" style="color: #3273dc;">growing-ai-like-a-child.github.io</a>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                    <!-- PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/vlm_gaze_paper.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (todo)</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (todo)</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/grow-ai-like-a-child/referential-gaze" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Analysis Code</span>
                  </a>
                </span>

            </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser section-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <strong>TL;DR:</strong> In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing.
      </h2>
      
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <source src="static/videos/teaser.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser section -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Probing the gaze referential understanding that emerges in Vision Language Models (VLMs) helps evaluate the kind of theory of mind they acquired, a critical socio-cognitive skill that affects how well they can understand humans and naturally interact with humans.
            In a controlled behavioral study, we curated a set of images with systematically manipulated difficulty and variability, evaluated 111 VLMs' abilities to infer gaze referents, and analyzed their performance using mixed-effect modeling.
          </p>
          <p>
            <strong>94 of the 111 VLMs fail to do better than random guessing</strong>, while humans achieve near ceiling accuracy (N=65).
            Although VLMs as a whole seem to struggle, as we zoom in to five of the top-tier VLMs that perform better than chance, we find that their performance declined with increasing task difficulty but varied only slightly with the specific prompt used and the gazer and objects in the scene.
            <strong>These behavioral patterns cannot be explained by considering them as stochastic parrots.</strong>
            Instead, they suggest that top-tier models' underlying inferences approach the gaze inference task in a meaningful way that is subject to the task difficulty, likely a combination of heuristics and guessing.
            This evidence suggests that VLMs are yet to become technologies that can naturally interact with humans, but the future remains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Key Findings Section -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">üîç Key Findings</h2>
        
        <div class="columns is-multiline">
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
              <h3 class="title is-4" style="color: white;">
                <i class="fas fa-chart-line"></i> Massive Performance Gap
              </h3>
              <p><strong>94 of 111 VLMs</strong> performed no better than random guessing (~ 42%), while humans achieved <strong>~92% accuracy</strong>. Even top-tier models like GPT-4o only reached ~50% accuracy. Are they simply guessing randomly?</p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);">
              <h3 class="title is-4">
                <i class="fas fa-eye"></i> Task Difficulty Effects
              </h3>
              <p>VLM performance (baseline-corrected) degrades significantly with increasing <strong>distance between objects</strong> and <strong>number of objects</strong>, but surprisingly shows no view angle effects (while humans do).</p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);">
              <h3 class="title is-4">
                <i class="fas fa-brain"></i> Not Random Guessing
              </h3>
              <p>Top-tier VLMs show <strong>systematic behavioral patterns</strong> (i.e., Task Difficulty Effects) that suggest meaningful computation rather than random responses, indicating some understanding of gaze direction and spatial relationships.</p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);">
              <h3 class="title is-4">
                <i class="fas fa-exclamation-triangle"></i> Scaling Limitations
              </h3>
              <p><strong>Model size and release date</strong> show minimal correlation with performance (R¬≤ < 0.05), suggesting fundamental architectural limitations rather than scale issues.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Stimuli carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Controlled Study</h2>
        
        <div id="stimuli-carousel" class="carousel results-carousel">
            <div class="item">
                <img src="static/images/stimuli/stimuli_v3.jpg"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        Controlled Variables: Systematic manipulation of View (left/right/front), Proximity (1-3 scale), #Objects (2-4), Objects (18 combinations of 9 items), and Gazer (2 actors) across 900 test stimuli. Subfigure (c) depicts stimuli with different numbers of objects on the table (with a Proximity value of 2). The gazer here is actor X.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/stimuli/stimuli_y.jpg"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        The gazer is actor Y.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_36.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_138.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_402.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_550.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_683.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_785.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_871.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_904.png"/>
            </div>
        </div>
        </div>
    </div>
    </section>
<!-- End Stimuli carousel -->


<!-- Survey carousel -->
<section class="hero is-small">
<div class="hero-body">
    <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Human Survey Interfaces</h2>
    
    <div id="survey-carousel" class="carousel results-carousel">
        <div class="item">
            <img src="static/images/survey_prompt.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
                The instruction page. We ask participants to press different buttons to make sure they read and follow the instructions.
            </div>
        </h3>
        </div>
        <div class="item">
            <img src="static/images/survey_warning.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            This page explains the presence of attention checks. All participants were paid regardless.
            </div>
        </h3>
        </div>
    
        <div class="item">
        <img src="static/images/survey_actor_Y.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            A question page where participants will click one of the buttons to make choices and proceed to the next question.
            </div>
        </h3>
        </div>
    </div>
    </div>
</div>
</section>
<!-- End Survey carousel -->

<!-- Results Section -->
<section class="section">
<div class="container is-max-desktop">
  <h2 class="title is-3 has-text-centered">üìä Experiments</h2>
  <div class="content has-text-justified">
    
    <h3 class="title is-4">Analysis A: Overall Performance Comparison</h3>
    
    <div class="box" style="background: #f8f9fa; padding: 2rem; margin: 2rem 0;">
      <div class="columns is-multiline">
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #f39c12;">~42%</div>
          <div class="has-text-weight-semibold">Random Baseline</div>
          <div class="is-size-7 has-text-grey">(Expected chance performance)</div>
        </div>
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #e74c3c;">94/111</div>
          <div class="has-text-weight-semibold">VLMs at Chance Level</div>
          <div class="is-size-7 has-text-grey">(Failed statistical significance test)</div>
        </div>
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #27ae60;">~92%</div>
          <div class="has-text-weight-semibold">Human Performance</div>
          <div class="is-size-7 has-text-grey">(Near ceiling accuracy)</div>
        </div>
      </div>
    </div>

    <h3 class="title is-4">Analysis B: Behavioral Patterns in Top-Tier VLMs and Humans</h3>
    
    <p>We conducted a pre-registered study focusing on five top-performing VLMs and humans to understand their behavioral patterns using mixed-effect logistic regression models. To correct for baseline due to the varying number of choices, we look at not the accuracy but the ratio between the odds (not probability) of accuracy and chance baseline.</p>
    
    <div class="columns is-multiline">
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üìê Proximity Effect</h4>
          <p><strong>Significant for 4/5 VLMs and humans:</strong> Performance degrades as objects get closer together. Unlike random guessing, this shows sensitivity to spatial difficulty.</p>
          <div style="background: #e8f4f8; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Statistical Finding:</strong> p < 0.05 for Gemini, GPT-4o, GLM-4V, InternLM (p = 0.15 for Qwen)
          </div>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üéØ Choice Effect</h4>
          <p><strong>Significant for all 5 VLMs and humans:</strong> Performance drops dramatically as number of candidate objects increases from 2 to 4.</p>
          <div style="background: #f0e8ff; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Statistical Finding:</strong> p < 0.001 for all tested VLMs, effect sizes ranging from -0.165 to -0.285
          </div>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 title is-5">üëÅÔ∏è View Effect</h4>
          <p><strong>No significant effect for VLMs</strong> but strong effect for humans (p < 0.001). Humans struggle with profile views while VLMs show no such pattern.</p>
          <div style="background: #fff2e8; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Key Insight:</strong> VLMs may use different visual cues than humans for gaze inference
          </div>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üîÑ Prompt Robustness</h4>
          <p><strong>Low prompt sensitivity:</strong> Only Gemini and Qwen showed minimal variance across 12 different prompt templates (variance < 0.1).</p>
          <div style="background: #e8f8e8; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Contrasts with:</strong> Previous findings of high prompt sensitivity in VLM gaze tasks
          </div>
        </div>
      </div>
    </div>

    <h3 class="title is-4">Summary</h3>
    
    <div style="overflow-x: auto;">
      <table class="table is-striped is-hoverable is-fullwidth">
        <thead>
          <tr style="background: #667eea; color: white;">
            <th>Model</th>
            <th>Overall Accuracy</th>
            <th>Above Chance?</th>
            <th>Proximity Effect</th>
            <th>Choice Effect</th>
            <th>Actor Sensitivity</th>
          </tr>
        </thead>
        <tbody>
          <tr style="background: #fff3cd;">
            <td><strong>Human Baseline</strong></td>
            <td><strong>~92%</strong></td>
            <td><strong>Yes (p < 0.001)</strong></td>
            <td><strong>Significant</strong></td>
            <td><strong>Significant</strong></td>
            <td><strong>Individual variation</strong></td>
          </tr>
          <tr>
            <td>GPT-4o</td>
            <td>~60%</td>
            <td>Yes (p < 0.002)</td>
            <td>Significant</td>
            <td>Significant</td>
            <td>Significant</td>
          </tr>
          <tr>
            <td>Gemini 1.5 Pro</td>
            <td>~55%</td>
            <td>Yes (p < 0.002)</td>
            <td>Significant</td>
            <td>Significant</td>
            <td>Not significant</td>
          </tr>
          <tr>
            <td>GLM-4V</td>
            <td>~52%</td>
            <td>Yes (p < 0.002)</td>
            <td>Significant</td>
            <td>Significant</td>
            <td>Significant</td>
          </tr>
          <tr>
            <td>InternLM-XComposer2</td>
            <td>~48%</td>
            <td>Yes (p < 0.002)</td>
            <td>Significant</td>
            <td>Significant</td>
            <td>Significant</td>
          </tr>
          <tr>
            <td>Qwen2.5-VL</td>
            <td>~45%</td>
            <td>Yes (p < 0.002)</td>
            <td>Not significant</td>
            <td>Significant</td>
            <td>Significant</td>
          </tr>
          <tr style="background: #f8d7da;">
            <td><em>Random Baseline</em></td>
            <td><em>~42%</em></td>
            <td><em>N/A</em></td>
            <td><em>None</em></td>
            <td><em>None</em></td>
            <td><em>None</em></td>
          </tr>
        </tbody>
      </table>
    </div>
    
    <p style="margin-top: 20px; font-size: 1rem;">
      <strong>Critical Finding:</strong> The systematic behavioral patterns observed in top-tier VLMs cannot be explained by random guessing. 
      Their performance degradation with task difficulty suggests they employ heuristics or approximations that work for easier cases but break down under challenging conditions.
      The lack of scaling effects (model size R¬≤ < 0.05, temporal improvements R¬≤ < 0.03) indicates fundamental architectural limitations rather than simply needing more parameters or training.
    </p>
  </div>
</div>
</section>

<!-- Implications Section -->
<section class="section hero is-light">
<div class="container is-max-desktop">
  <h2 class="title is-3 has-text-centered">üí° Implications & Future Work</h2>
  
  <div class="columns is-multiline">
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #e74c3c;">
        <h3 class="title is-4">
          <i class="fas fa-exclamation-triangle" style="color: #ff6b6b;"></i> Theory of Mind Limitations
        </h3>
        <ul>
          <li><strong>Fundamental Gap:</strong> VLMs lack the gaze inference abilities that bootstrap human social cognition</li>
          <li><strong>Scaling Won't Fix It:</strong> Model size and time show no correlation with performance</li>
          <li><strong>Different Visual Processing:</strong> VLMs don't show human-like view angle effects</li>
          <li><strong>Heuristic Breakdown:</strong> Performance collapses under increased spatial complexity</li>
        </ul>
      </div>
    </div>
    
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #48c774;">
        <h3 class="title is-4">
          <i class="fas fa-lightbulb" style="color: #ffdd57;"></i> Research Directions
        </h3>
        <ul>
          <li><strong>Mechanistic Understanding:</strong> Investigate how top-tier VLMs approximate gaze inference</li>
          <li><strong>Specialized Training:</strong> Develop gaze-aware training paradigms inspired by human development</li>
          <li><strong>Architectural Innovation:</strong> Design models that can handle spatial relationships robustly</li>
          <li><strong>Controlled Studies:</strong> More behavioral probes beyond benchmark scores</li>
        </ul>
      </div>
    </div>
    
    <div class="column is-full">
      <div class="box" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
        <h3 class="title is-4" style="color: white;">
          <i class="fas fa-microscope"></i> Key Methodological Contribution
        </h3>
        <p style="font-size: 1.1rem; margin-bottom: 1rem;">
          This work demonstrates how <strong>controlled behavioral studies</strong> can reveal insights that pure benchmarking cannot. 
          By systematically manipulating difficulty factors and using mixed-effect modeling, we moved beyond simple accuracy scores to understand the <em>kind</em> of computation underlying VLM responses.
        </p>
        <p style="font-size: 1.1rem;">
          Our findings suggest that top-tier VLMs are <strong>not simply stochastic parrots</strong> but employ imperfect heuristics that approximate meaningful spatial reasoning for easier cases. 
          This provides a more nuanced understanding of current VLM capabilities and limitations in social cognition tasks.
        </p>
      </div>
    </div>
    
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #9b59b6;">
        <h3 class="title is-4">
          <i class="fas fa-question-circle" style="color: #9b59b6;"></i> Open Questions
        </h3>
        <ul>
          <li>What mechanisms enable top-tier VLMs to approximate gaze inference?</li>
          <li>Can human-inspired training improve spatial reasoning?</li>
          <li>How do different visual cues contribute to VLM vs. human performance?</li>
          <li>Will models trained on gaze data from infancy show better ToM?</li>
        </ul>
      </div>
    </div>
    
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #f39c12;">
        <h3 class="title is-4">
          <i class="fas fa-road" style="color: #f39c12;"></i> Path Forward
        </h3>
        <ul>
          <li><strong>Beyond Benchmarks:</strong> More controlled behavioral studies of VLM cognition</li>
          <li><strong>Developmental AI:</strong> Training paradigms inspired by human cognitive development</li>
          <li><strong>Mechanistic Analysis:</strong> Understanding the computational basis of observed behaviors</li>
          <li><strong>Real-world Applications:</strong> Improving human-AI interaction through better social understanding</li>
        </ul>
      </div>
    </div>
  </div>
</div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{vlmGaze2025,
  title={Can Vision Language Models Infer Human Gaze Direction? A Controlled Study},
  author={Zhang, Zory and Feng, Pinyuan and Wang, Bingyang and Zhao, Tianwei and Yu, Suyang and Gao, Qingying and Deng, Hokin and Ma, Ziqiao and Li, Yijiang and Luo, Dezhi},
  year={2025},
  eprint={2504.16060 TODO},
  archivePrefix={arXiv},
  primaryClass={cs.CL TODO},
  url={https://arxiv.org/abs/2504.16060 TODO}, 
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p class="has-text-centered">
          <strong>üå± GrowAI Team</strong><br>
          <a href="https://growing-ai-like-a-child.github.io/" target="_blank">growing-ai-like-a-child.github.io</a>
        </p>
        <p style="font-size: 0.9rem;">
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>

<script>
$(document).ready(function() {
    // Check for click events on the navbar burger icon

    var options = {
            slidesToScroll: 1,
            slidesToShow: 1,
            loop: true,
            infinite: true,
            autoplay: false,
    }

    // Initialize all div with carousel class
    var carousels = bulmaCarousel.attach('.carousel', options);
    
    bulmaSlider.attach();
})
</script>

</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Can Vision Language Models Infer Human Gaze Direction? | VLMgaze Research</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .hero {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 60px 40px;
            margin-bottom: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
            text-align: center;
        }

        .hero h1 {
            font-size: 3.5rem;
            font-weight: 700;
            margin-bottom: 20px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            line-height: 1.2;
        }

        .hero .subtitle {
            font-size: 1.5rem;
            color: #666;
            margin-bottom: 30px;
            font-weight: 400;
        }

        .hero .tagline {
            font-size: 1.2rem;
            color: #888;
            margin-bottom: 40px;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .authors {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 15px;
            margin-bottom: 30px;
        }

        .author {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 8px 16px;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 500;
        }

        .affiliation {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 30px;
        }

        .cta-buttons {
            display: flex;
            gap: 20px;
            justify-content: center;
            flex-wrap: wrap;
        }

        .btn {
            padding: 15px 30px;
            border: none;
            border-radius: 50px;
            font-size: 1.1rem;
            font-weight: 600;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 10px;
            transition: all 0.3s ease;
            cursor: pointer;
        }

        .btn-primary {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
        }

        .btn-secondary {
            background: white;
            color: #667eea;
            border: 2px solid #667eea;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);
        }

        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin-bottom: 40px;
        }

        .feature-card {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
            transition: transform 0.3s ease;
        }

        .feature-card:hover {
            transform: translateY(-5px);
        }

        .feature-card h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 15px;
            color: #333;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .feature-card p {
            color: #666;
            font-size: 1rem;
            line-height: 1.6;
        }

        .feature-icon {
            width: 40px;
            height: 40px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 1.2rem;
        }

        .results-section {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 40px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        .results-section h2 {
            font-size: 2.5rem;
            font-weight: 700;
            text-align: center;
            margin-bottom: 30px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .figure-container {
            text-align: center;
            margin: 30px 0;
        }

        .figure-container img {
            max-width: 100%;
            border-radius: 10px;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .figure-caption {
            font-style: italic;
            color: #666;
            margin-top: 15px;
            font-size: 1rem;
        }

        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }

        .stat-card {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
        }

        .stat-number {
            font-size: 2.5rem;
            font-weight: 700;
            display: block;
        }

        .stat-label {
            font-size: 1rem;
            opacity: 0.9;
        }

        .footer {
            text-align: center;
            color: rgba(255, 255, 255, 0.8);
            padding: 40px 0;
        }

        .footer a {
            color: rgba(255, 255, 255, 0.9);
            text-decoration: none;
            font-weight: 500;
        }

        .footer a:hover {
            color: white;
        }

        @media (max-width: 768px) {
            .hero h1 {
                font-size: 2.5rem;
            }
            
            .hero .subtitle {
                font-size: 1.2rem;
            }
            
            .container {
                padding: 15px;
            }
            
            .hero {
                padding: 40px 20px;
            }
            
            .cta-buttons {
                flex-direction: column;
                align-items: center;
            }
        }

        .highlight {
            background: linear-gradient(135deg, #ffecd2, #fcb69f);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #ff6b6b;
        }

        .team-badge {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 10px 20px;
            border-radius: 25px;
            display: inline-block;
            margin: 20px 0;
            font-weight: 600;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="hero">
            <h1>Can Vision Language Models Infer Human Gaze Direction?</h1>
            <p class="subtitle">A Controlled Study</p>
            <p class="tagline">Investigating the gap between cutting-edge AI vision models and human-level gaze inference capabilities through rigorous experimental design</p>
            
            <div class="authors">
                <span class="author">Zory Zhang†</span>
                <span class="author">Pinyuan Feng†</span>
                <span class="author">Bingyang Wang</span>
                <span class="author">Tianwei Zhao</span>
                <span class="author">Suyang Yu</span>
                <span class="author">Qingying Gao</span>
                <span class="author">Hokin Deng*</span>
                <span class="author">Ziqiao Ma*</span>
                <span class="author">Yijiang Li*</span>
                <span class="author">Dezhi Luo*</span>
            </div>
            
            <div class="team-badge">
                <i class="fas fa-users"></i> GrowAI Team
            </div>
            
            <div class="affiliation">
                Brown University • Columbia University • Emory University • Johns Hopkins University<br>
                University of Washington • Carnegie Mellon University • University of Michigan • UC San Diego
            </div>
            
            <div class="cta-buttons">
                <a href="#" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i>
                    Read Paper
                </a>
                <a href="https://growing-ai-like-a-child.github.io/" class="btn btn-secondary">
                    <i class="fas fa-globe"></i>
                    Visit Project
                </a>
                <a href="#" class="btn btn-secondary">
                    <i class="fab fa-github"></i>
                    Code & Data
                </a>
            </div>
        </div>

        <div class="features">
            <div class="feature-card">
                <h3>
                    <div class="feature-icon">
                        <i class="fas fa-eye"></i>
                    </div>
                    Gaze Inference Challenge
                </h3>
                <p>We investigate whether state-of-the-art Vision Language Models can understand and predict human gaze direction in controlled scenarios, revealing a significant performance gap compared to human capabilities.</p>
            </div>
            
            <div class="feature-card">
                <h3>
                    <div class="feature-icon">
                        <i class="fas fa-microscope"></i>
                    </div>
                    Rigorous Methodology
                </h3>
                <p>Using controlled experimental design with mixed-effect modeling, we systematically evaluate multiple VLMs against human baselines to understand the factors affecting gaze inference performance.</p>
            </div>
            
            <div class="feature-card">
                <h3>
                    <div class="feature-icon">
                        <i class="fas fa-robot"></i>
                    </div>
                    VLM Evaluation
                </h3>
                <p>Comprehensive testing of top-tier Vision Language Models including GPT-4V, Claude-3, Gemini, and others on gaze referential inference tasks with varying complexity and conditions.</p>
            </div>
            
            <div class="feature-card">
                <h3>
                    <div class="feature-icon">
                        <i class="fas fa-users"></i>
                    </div>
                    Human Baseline
                </h3>
                <p>Extensive human subject studies with IRB approval, fair compensation, and rigorous data collection protocols to establish ground truth performance benchmarks.</p>
            </div>
        </div>

        <div class="results-section">
            <h2><i class="fas fa-chart-line"></i> Key Findings</h2>
            
            <div class="highlight">
                <h3><i class="fas fa-exclamation-triangle"></i> Performance Gap Discovered</h3>
                <p>Our study reveals a significant performance gap between state-of-the-art Vision Language Models and human performance in gaze direction inference tasks, highlighting an important limitation in current AI vision capabilities.</p>
            </div>

            <div class="stats">
                <div class="stat-card">
                    <span class="stat-number">Multiple</span>
                    <span class="stat-label">VLMs Tested</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">Rigorous</span>
                    <span class="stat-label">Statistical Analysis</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">IRB</span>
                    <span class="stat-label">Approved Study</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">Open</span>
                    <span class="stat-label">Source Data</span>
                </div>
            </div>

            <div class="figure-container">
                <h3>Experimental Setup & Results</h3>
                <p class="figure-caption">
                    Our controlled study design evaluates Vision Language Models on gaze referential inference tasks,
                    revealing systematic differences in performance across various conditions and model architectures.
                </p>
            </div>

            <div style="background: #f8f9fa; padding: 25px; border-radius: 10px; margin: 30px 0;">
                <h3><i class="fas fa-lightbulb"></i> Research Impact</h3>
                <ul style="list-style: none; padding: 0;">
                    <li style="margin: 10px 0;"><i class="fas fa-check" style="color: #28a745; margin-right: 10px;"></i> <strong>Identifies limitations</strong> in current VLM gaze understanding capabilities</li>
                    <li style="margin: 10px 0;"><i class="fas fa-check" style="color: #28a745; margin-right: 10px;"></i> <strong>Provides benchmarks</strong> for future model development and evaluation</li>
                    <li style="margin: 10px 0;"><i class="fas fa-check" style="color: #28a745; margin-right: 10px;"></i> <strong>Offers insights</strong> into human-AI vision processing differences</li>
                    <li style="margin: 10px 0;"><i class="fas fa-check" style="color: #28a745; margin-right: 10px;"></i> <strong>Contributes</strong> to understanding of social cognition in AI systems</li>
                </ul>
            </div>
        </div>

        <div class="results-section">
            <h2><i class="fas fa-tools"></i> Methodology & Approach</h2>
            
            <div class="features">
                <div class="feature-card">
                    <h3>
                        <div class="feature-icon">
                            <i class="fas fa-cogs"></i>
                        </div>
                        Controlled Variables
                    </h3>
                    <p>Systematic manipulation of gaze angle, proximity, and number of target candidates to understand factors affecting VLM performance in gaze inference tasks.</p>
                </div>
                
                <div class="feature-card">
                    <h3>
                        <div class="feature-icon">
                            <i class="fas fa-chart-bar"></i>
                        </div>
                        Statistical Analysis
                    </h3>
                    <p>Mixed-effect modeling and rigorous statistical testing to ensure reliable conclusions about VLM capabilities and human-AI performance differences.</p>
                </div>
                
                <div class="feature-card">
                    <h3>
                        <div class="feature-icon">
                            <i class="fas fa-database"></i>
                        </div>
                        Open Research
                    </h3>
                    <p>Full dataset, code, and experimental protocols made available for reproducibility and future research in VLM evaluation and gaze inference studies.</p>
                </div>
                
                <div class="feature-card">
                    <h3>
                        <div class="feature-icon">
                            <i class="fas fa-balance-scale"></i>
                        </div>
                        Ethical Standards
                    </h3>
                    <p>IRB-approved human subject research with fair compensation, informed consent, and adherence to NeurIPS ethical guidelines throughout the study.</p>
                    </div>
            </div>
        </div>
    </div>

    <div class="footer">
        <p>
            <strong>NeurIPS 2025 Submission</strong><br>
            For more information, visit the 
            <a href="https://growing-ai-like-a-child.github.io/">GrowAI Team website</a>
        </p>
        <p style="margin-top: 20px; font-size: 0.9rem; opacity: 0.7;">
            © 2024 GrowAI Team. All rights reserved.
        </p>
    </div>

    <script>
        // Add smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });

        // Add animation on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.feature-card, .results-section').forEach(el => {
            el.style.opacity = '0';
            el.style.transform = 'translateY(20px)';
            el.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            observer.observe(el);
        });
    </script>
</body>
</html> 
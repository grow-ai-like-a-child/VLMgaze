<!DOCTYPE html>
<html>
<head>
  <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëÅÔ∏è</text></svg>">
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing.">
  <meta property="og:title" content="Can Vision Language Models Infer Human Gaze Direction? A Controlled Study"/>
  <meta property="og:description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing."> 
  <meta property="og:url" content="https://grow-ai-like-a-child.github.io/VLMgaze/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser_v2.jpg"/>
  <meta property="og:image:width" content="1661"/>
  <meta property="og:image:height" content="649"/>

  <meta name="twitter:title" content="VLMs Cannot Yet Infer Human Gaze Direction">
  <meta name="twitter:description" content="In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' response that suggest they are not simply guessing."> 
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser_v2.jpg">
  <meta name="twitter:card" content="static/images/teaser_v2.jpg">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Models, Gaze Inference, Human Vision, Spatial Reasoning, AI Evaluation">
  <meta name="author" content="Zory Zhang, Pinyuan Feng, Bingyang Wang, Tianwei Zhao, Suyang Yu, Qingying Gao, Hokin Deng, Ziqiao Ma, Yijiang Li, Dezhi Luo (GrowAI Team)">
  <meta name="robots" content="index, follow">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Can Vision Language Models Infer Human Gaze Direction? A Controlled Study</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëÅÔ∏è</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.0/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.0/dist/js/bulma-slider.min.js"></script>
  
  <style>
    #survey-carousel {
        overflow: hidden !important;
    }
    
    .results-carousel {
        margin-bottom: 3rem !important;
        overflow: hidden !important;
    }
    
    .results-carousel .slider-pagination {
        margin-top: 2rem !important;
    }

    .results-carousel .item {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        margin: 5px;
        overflow: hidden;
        padding: 20px;
        font-size: 0;
    }

    .results-carousel video {
        margin: 0;
    }
    
    .results-carousel .item > img {
        margin: 0 auto;
        max-height: 450px;
        object-fit: contain;
    }

    .results-carousel .item .subtitle {
    margin-top: 15px;
    margin-bottom: 3rem !important; /* Add more space below subtitle */
    }

    .slider-pagination .slider-page {
        background: #000000;
    }

    .hero-body {
      padding: 3rem 1.5rem;
    }

    .title.is-1 {
      font-size: 2.5rem !important;
    }

    @media screen and (min-width: 1024px) {
      .title.is-1 {
        font-size: 3rem !important;
      }
    }

    .publication-title {
      margin-bottom: 1rem !important;
    }

    .author-block {
      margin-right: 1rem;
    }
  </style>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 20px;">
              <h1 class="title is-1 publication-title">üëÅÔ∏è Can Vision Language Models Infer Human Gaze Direction? A Controlled Study</h1>
            </div>
            <h4 class="title is-size-4 publication-title">Preprint under review</h4>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zoryzhang.notion.site" target="_blank">Zory Zhang</a><sup>‚Ä†,1</sup>,
              </span>
              <span class="author-block">
                <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/pinyuan-feng" target="_blank">Pinyuan Feng</a><sup>‚Ä†,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=%7EBingyang_Wang2" target="_blank">Bingyang Wang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Tianwei_Zhao1" target="_blank">Tianwei Zhao</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://stat.uw.edu/people/suyang-yu" target="_blank">Suyang Yu</a><sup>5</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://kaiagaoqy.github.io/personalwebsite" target="_blank">Qingying Gao</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://hokindeng.github.io" target="_blank">Hokin Deng</a><sup>*,6</sup>,
              </span>
              <span class="author-block">
                <a href="https://mars-tin.github.io" target="_blank">Ziqiao Ma</a><sup>*,7</sup>,
              </span>
              <span class="author-block">
                <a href="https://williamium3000.github.io" target="_blank">Yijiang Li</a><sup>*,8</sup>,
              </span>
              <span class="author-block">
                <a href="https://ihzedoul.com" target="_blank">Dezhi Luo</a><sup>*,7</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Brown University,
                <sup>2</sup>Columbia University,
                <sup>3</sup>Emory University,
                <sup>4</sup>Johns Hopkins University,<br>
                <sup>5</sup>University of Washington,
                <sup>6</sup>Carnegie Mellon University,
                <sup>7</sup>University of Michigan,
                <sup>8</sup>UC San Diego
              </span>
              <span class="eql-cntrb"><small><br><sup>‚Ä†</sup>Equal Contribution, <sup>*</sup>Equal Advising</small></span>
              <br>
              <span class="author-block" style="color: #4a4a4a; margin-top: 10px;">
                <img src="static/images/growai.png" alt="GrowAI Team Logo" style="max-height: 100px; margin-top: 10px;"><br><strong>GrowAI Team</strong> | <a href="https://growing-ai-like-a-child.github.io/" target="_blank" style="color: #3273dc;">growing-ai-like-a-child.github.io</a>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                    <!-- PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/vlm_gaze_paper.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (todo)</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (todo)</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/grow-ai-like-a-child/referential-gaze" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Analysis Code</span>
                  </a>
                </span>

            </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser section-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <strong>TL;DR:</strong> In a task involving inference of human gaze direction, our controlled study reveals a substantial performance gap between top-tier Vision-Language Models (VLMs) and humans, as well as behavioral patterns in VLMs' responses that suggest they are not simply guessing.
      </h2>
      
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <source src="static/videos/teaser.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser section -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Probing the gaze-referential understanding that emerges in Vision Language Models (VLMs) helps evaluate the kind of theory of mind they have acquired, a critical socio-cognitive skill that affects how well they can understand humans and naturally interact with humans.
            In a controlled study, we curated images with systematically manipulated difficulty and variability, evaluated 111 VLMs' abilities to infer gaze referents, and analyzed behaviors using mixed-effect modeling.
          </p>
          <p>
            <strong>94 of the 111 VLMs fail to do better than random guessing</strong>, while humans achieve near-ceiling accuracy (N=65).
            Although most VLMs struggle, when we zoom in on five of the top-tier VLMs that perform better than chance, we find that their performance declined with increasing task difficulty but varied only slightly with the specific prompt used and the objects in the scene.
            <strong>These behavioral features cannot be explained by considering them as stochastic parrots.</strong>
            Instead, they suggest that their underlying inferences approach the problem in a meaningful way that is subject to task difficulty, likely using a combination of heuristics and guessing.
            This suggests that VLMs have yet to become technologies that can naturally interact with humans, but the potential remains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Key Findings Section -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">üîç Key Findings</h2>
        
        <div class="columns is-multiline">
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
              <h3 class="title is-4" style="color: white;">
                <i class="fas fa-chart-line"></i> Massive Performance Gap
              </h3>
              <p><strong style="color: white;">94 of 111 VLMs</strong> performed no better than random guessing (~42%), while humans achieved ~91% accuracy. VLMs responded with every possible option <strong style="color: white;">almost equally often</strong>. Even top-tier VLMs like GPT-4o only reached ~50% accuracy. Are they simply guessing randomly?</p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);">
              <h3 class="title is-4">
                <i class="fas fa-exclamation-triangle"></i> Scaling Limitations
              </h3>
              <p>VLM size and release date show minimal correlation with performance (both R¬≤ < 0.03), suggesting <strong>fundamental architectural limitations rather than scale issues</strong>. The pattern of errors they made is also quite different from that of humans and does not become more similar as VLMs get bigger (R¬≤ < 0.1).</p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);">
              <h3 class="title is-4">
                <i class="fas fa-eye"></i> Task Difficulty Effects
              </h3>
              <p>The performance of the 5 top-tier VLMs (baseline-corrected) degrades significantly with increasing proximity between objects and number of objects, but surprisingly shows no view angle effects (while humans do). In general, this means <strong>their performance becomes closer to random guessing as difficulty increases.</strong></p>
            </div>
          </div>
          
          <div class="column is-half">
            <div class="box" style="height: 100%; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);">
              <h3 class="title is-4">
                <i class="fas fa-brain"></i> Not Random Guessing
              </h3>
              <p>Task Difficulty Effects suggest meaningful computation rather than random responses. Top-tier VLMs likely employ heuristics (or approximations) that work to some extent for easier cases but break down under challenging conditions, an indication of <strong>partial understanding of gaze direction and spatial relationships.</strong></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Stimuli carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Controlled Study & Stimuli Examples</h2>
        
        <div id="stimuli-carousel" class="carousel results-carousel">
            <div class="item">
                <img src="static/images/stimuli/stimuli_v4.jpg"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        Systematic manipulation of controlled variables: View (left/right/front), Proximity (1-3 scale), #Objects (2-4), Objects (18 combinations of 9 items), and Gazer (2 actors) across 900 test stimuli. The gazer here is Actor X.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/stimuli/stimuli_y_v2.jpg"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        The gazer is Actor Y.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_36.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_138.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_402.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_550.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_683.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_785.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_871.png"/>
            </div>
            <div class="item">
                <img src="static/images/stimuli/gaze_904.png"/>
            </div>
        </div>
        </div>
    </div>
    </section>
<!-- End Stimuli carousel -->


<!-- Survey carousel -->
<section class="hero is-small">
<div class="hero-body">
    <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Human Survey Interfaces (via JsPsych + Prolific)</h2>
    
    <div id="survey-carousel" class="carousel results-carousel">
        <div class="item">
            <img src="static/images/survey_prompt.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
                The instruction page, after being put into the fullscreen mode. We ask participants to press different buttons to ensure they read and follow the instructions.
            </div>
        </h3>
        </div>
        <div class="item">
            <img src="static/images/survey_warning.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            This page explains the presence of attention checks. All participants were paid regardless.
            </div>
        </h3>
        </div>
    
        <div class="item">
        <img src="static/images/survey_actor_Y.png"/>
        <h3 class="subtitle has-text-centered">
            <div style="font-size: 1rem; margin-top: 10px;">
            A question page where participants click one of the buttons to make their choice and proceed to the next question.
            </div>
        </h3>
        </div>
    </div>
    </div>
</div>
</section>
<!-- End Survey carousel -->

<!-- Results Section -->
<section class="section">
<div class="container is-max-desktop">
  <h2 class="title is-3 has-text-centered">üìä Analyses</h2>
  <div class="content has-text-justified">
    
    <h3 class="title is-4">Analysis A: Overall Performance Comparison</h3>
    
    <div class="box" style="padding: 2rem; margin: 2rem 0;">
      <div class="columns is-multiline">
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #f39c12;">~42%</div>
          <div class="has-text-weight-semibold">Random-Guessing Baseline</div>
          <div class="is-size-7 has-text-grey">(Expected chance performance)</div>
        </div>
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #e74c3c;">94/111</div>
          <div class="has-text-weight-semibold">VLMs at Chance Level</div>
          <div class="is-size-7 has-text-grey">(Failed statistical significance test)</div>
        </div>
        <div class="column is-one-third has-text-centered">
          <div class="has-text-weight-bold is-size-1" style="color: #27ae60;">~91%</div>
          <div class="has-text-weight-semibold">Human Performance</div>
          <div class="is-size-7 has-text-grey">(Near-ceiling accuracy)</div>
        </div>
      </div>
    </div>

    <div class="columns">
      <div class="column is-three-fifths">
        <div class="box">
          <h4 class="title is-5">üîç Confusion Matrix Analysis</h4>
          <div style="text-align: center; margin-bottom: 1rem;">
            <img src="static/images/confusion_matrix_v5.jpg" alt="Confusion matrices comparison" style="max-width: 100%; height: auto;">
          </div>
          <p>Humans show strong diagonal patterns indicating consistent correct responses, while VLMs produce nearly uniform distributions across all choices‚Äî<strong>a pattern consistent with a random-guessing account</strong> that led to our initial speculation‚Äîare they guessing?</p>
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="box">
          <h4 class="title is-5">üìà Scaling Analysis</h4>
          <div style="text-align: center; margin-bottom: 1rem;">
            <img src="static/images/ranking_size.png" style="max-width: 100%; height: auto;">
          </div>
          <p>VLM size (and release date) show minimal correlation with accuracy (both R¬≤ < 0.03), suggesting <strong>core architectural limitations</strong>.</p>
        </div>
      </div>
    </div>

    <h3 class="title is-4">Analysis B: Behavioral Patterns in Top-Tier VLMs and Humans</h3>
    
    <p>We conducted a pre-registered study focusing on five top-performing VLMs and humans to understand their behavioral patterns using mixed-effects logistic regression models. To correct for baseline due to the varying number of choices, we examine not the accuracy but <strong>the ratio between the odds of accuracy and the chance baseline</strong>. The hypothesis that heuristics used in easier cases break down for harder cases stems from this degradation pattern‚Äîperformance moves closer to baseline as difficulty increases.</p>
    
    <div class="columns is-multiline">
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üìê Proximity Effect</h4>
          <p><strong>Significant for 4/5 VLMs and humans:</strong> Baseline-correct performance degrades as objects get closer together (p < 0.05 for Gemini, GPT-4o, GLM-4V, InternLM; p = 0.15 for Qwen).</p>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üéØ Choice Effect</h4>
          <p><strong>Significant for all 5 VLMs and humans:</strong> Baseline-correct performance drops dramatically (p < 0.001) as the number of candidate objects increases from 2 to 3 and 4. Effect sizes ranging from -0.165 to -0.285.</p>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üëÅÔ∏è View Effect</h4>
          <p>Human baseline-correct performance degrades when viewing the gazer's side profile (p < 0.001), whereas VLMs show no significant effect.</p>
          <div style="background: #fff2e8; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Key Insight:</strong> VLM may rely on head orientation rather than eye gaze direction, making them less sensitive to side-views that increase eye direction geometric ambiguity and hinder counterfactual reasoning in human observers.
          </div>
        </div>
      </div>
      
      <div class="column is-half">
        <div class="box">
          <h4 class="title is-5">üîÑ Sensitivity Analysis</h4>
          <p>While the number of levels for gazers (2) is too small to conclude sensitivity, there is minimal variance across prompts (small) and object combinations (almost zero).</p>
          <div style="background: #e8f8e8; padding: 1rem; border-radius: 5px; margin-top: 1rem;">
            <strong>Implication:</strong> The emergent inference VLMs have not only depends on the task difficulty but also can abstract away from surface-level variations, suggesting a meaningful computation rather than random guessing.
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<!-- Implications Section -->
<section class="section hero is-light">
<div class="container is-max-desktop">
  <h2 class="title is-3 has-text-centered">üí° Implications & Future Work</h2>
  
  <div class="columns is-multiline">
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #e74c3c;">
        <h3 class="title is-4">
          <i class="fas fa-exclamation-triangle" style="color: #ff6b6b;"></i> Implications for AI Development
        </h3>
        <ul>
          <li><strong>Gaze Inference Deficits:</strong> The problem might lies at not picking up eye gaze for inferring looking directions.</li>
          <li><strong>Theory of Mind and Human-AI Interaction:</strong> VLMs lack fundamental gaze inference abilities that bootstrap human social cognition and they are not yet ready for natural collaboration requiring social understanding.</li>
          <li><strong>Beyond Scaling:</strong> Current paradigms may be insufficient‚Äîmore parameters won't solve this gap.</li>
        </ul>
      </div>
    </div>
    
    <div class="column is-half">
      <div class="box" style="border-left: 4px solid #9b59b6;">
        <h3 class="title is-4">
          <i class="fas fa-question-circle" style="color: #9b59b6;"></i> Open Questions
        </h3>
        <ul>
          <li><strong>1. What is the mechanism of approximation</strong> in top-tier VLMs, and <strong>how does it emerge from the data?</strong></li>
          <li><strong>2. What roles do VLMs attach to gaze</strong> (e.g., the referential nature of gaze, using gaze to resolve conversational referent ambiguity)?</li>
          <li>3. To have AI that learn like humans and acquire Theory of Mind from naturalistic data, <strong>will a curriculum that encourages early development of gaze-referential understanding be helpful?</strong></li>
        </ul>
      </div>
    </div>
    
    <div class="column is-full">
      <div class="box" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
        <h3 class="title is-4" style="color: white;">
          <i class="fas fa-microscope"></i> Methodological Contribution
        </h3>
        <p style="font-size: 1.1rem;">
          This work demonstrates how <strong>controlled behavioral studies</strong> can reveal insights that pure benchmarking cannot. 
          By systematically manipulating difficulty factors and using mixed-effects modeling, we moved beyond simple accuracy scores to constrain hypotheses about VLMs' underlying gaze inference.
        </p>
      </div>
    </div>
  </div>
</div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{vlmGaze2025,
  title={Can Vision Language Models Infer Human Gaze Direction? A Controlled Study},
  author={Zhang, Zory and Feng, Pinyuan and Wang, Bingyang and Zhao, Tianwei and Yu, Suyang and Gao, Qingying and Deng, Hokin and Ma, Ziqiao and Li, Yijiang and Luo, Dezhi},
  year={2025},
  eprint={2504.16060 TODO},
  archivePrefix={arXiv},
  primaryClass={cs.CL TODO},
  url={https://arxiv.org/abs/2504.16060 TODO}, 
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p class="has-text-centered">
            <img src="static/images/growai.png" alt="GrowAI Team Logo" style="max-height: 200px; margin-top: 10px;"><br><strong>GrowAI Team</strong> | <a href="https://growing-ai-like-a-child.github.io/" target="_blank" style="color: #3273dc;">growing-ai-like-a-child.github.io</a>
        </p>
        <p style="font-size: 0.9rem;">
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>

<script>
$(document).ready(function() {
    // Check for click events on the navbar burger icon

    var options = {
            slidesToScroll: 1,
            slidesToShow: 1,
            loop: true,
            infinite: true,
            autoplay: false,
    }

    // Initialize all div with carousel class
    var carousels = bulmaCarousel.attach('.carousel', options);
    
    bulmaSlider.attach();
})
</script>

</body>
</html>